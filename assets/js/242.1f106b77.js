(window.webpackJsonp=window.webpackJsonp||[]).push([[242],{514:function(t,r,_){"use strict";_.r(r);var a=_(14),e=Object(a.a)({},(function(){var t=this,r=t._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("ul",[r("li",[r("a",{attrs:{href:"#1-alphago%E4%BB%8B%E7%BB%8D"}},[t._v("1. AlphaGo介绍")]),t._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"#11-%E4%B8%A4%E7%AF%87%E8%AE%BA%E6%96%87"}},[t._v("1.1. 两篇论文")])]),t._v(" "),r("li",[r("a",{attrs:{href:"#12-%E6%B7%B1%E5%BA%A6%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C-p%CF%83--supervised-learning-policy-network"}},[t._v("1.2. 深度策略网络 pσ : (Supervised Learning Policy Network)")])]),t._v(" "),r("li",[r("a",{attrs:{href:"#13-%E6%B7%B1%E5%BA%A6%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C-p%CF%81--reinforcement-learning-policy-network"}},[t._v("1.3. 深度策略网络 pρ : (Reinforcement Learning Policy Network)")])]),t._v(" "),r("li",[r("a",{attrs:{href:"#14-%E6%B7%B1%E5%BA%A6%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9Cp%CF%80-rollout-policy-network"}},[t._v("1.4. 深度策略网络pπ: (Rollout Policy Network)")])]),t._v(" "),r("li",[r("a",{attrs:{href:"#15-%E6%B7%B1%E5%BA%A6%E4%BC%B0%E5%80%BC%E7%BD%91%E7%BB%9Cv%CE%B8-rollout-policy-network"}},[t._v("1.5. 深度估值网络vθ: (Rollout Policy Network)")])]),t._v(" "),r("li",[r("a",{attrs:{href:"#16-%E4%B8%8B%E6%A3%8B%E6%96%B9%E6%B3%95----%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2-monte-carlo-tree-search"}},[t._v("1.6. 下棋方法 -- 蒙特卡洛树搜索 （Monte Carlo Tree Search）：")])]),t._v(" "),r("li",[r("a",{attrs:{href:"#17-alphago-zero-%E7%9A%84%E6%94%B9%E8%BF%9B"}},[t._v("1.7. AlphaGo Zero 的改进")])])])])]),t._v(" "),r("h2",{attrs:{id:"_1-alphago介绍"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-alphago介绍"}},[t._v("#")]),t._v(" 1. AlphaGo介绍")]),t._v(" "),r("h3",{attrs:{id:"_1-1-两篇论文"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-两篇论文"}},[t._v("#")]),t._v(" 1.1. 两篇论文")]),t._v(" "),r("ol",[r("li",[t._v("David Silver et al., Mastering the Game of Go with Deep Neural Networks and Tree Search, Nature, 2015.")]),t._v(" "),r("li",[t._v("David Silver et al., Mastering the Game of Go without Human Knowledge,Nature, 2017.")])]),t._v(" "),r("p",[t._v("三个深度策略网络 (Policy Networks),一个深度估值网络(Value Network)")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606155459_f78b8aebe49f.png",alt:"img"}})]),t._v(" "),r("h3",{attrs:{id:"_1-2-深度策略网络-pσ-supervised-learning-policy-network"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-深度策略网络-pσ-supervised-learning-policy-network"}},[t._v("#")]),t._v(" 1.2. 深度策略网络 pσ : (Supervised Learning Policy Network)")]),t._v(" "),r("ul",[r("li",[r("strong",[t._v("输入：当前棋盘状态")]),t._v("。")]),t._v(" "),r("li",[r("strong",[t._v("输出：下一步的走法")]),t._v("。")]),t._v(" "),r("li",[t._v("训练数据： "),r("strong",[t._v("KGS Go SERVER上的 三亿个样本")]),t._v("。")]),t._v(" "),r("li",[t._v("网络设置： 13层深度网络。")]),t._v(" "),r("li",[t._v("输入的特征如下图")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606160023_850a11766fce.png",alt:"img"}})]),t._v(" "),r("ul",[r("li",[t._v("结果："),r("strong",[t._v("57%正确率，3ms一步")])])]),t._v(" "),r("p",[t._v("这里的正确率指，通过棋盘的状态预测下一步走棋，"),r("strong",[t._v("判断是否和高手下的是否一致")]),t._v("。57%是很高的数字，因为上一次这样走、下一次还这样走的可能性比较低。")]),t._v(" "),r("p",[t._v("具体参数如下：")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606155854_2d61da8e981f.png",alt:"img"}})]),t._v(" "),r("p",[t._v("优化分析 ：")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606160104_ea06efab911b.png",alt:"img"}})]),t._v(" "),r("p",[t._v("棋盘特征：")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606160151_94cf83f97332.png",alt:"img"}})]),t._v(" "),r("h3",{attrs:{id:"_1-3-深度策略网络-pρ-reinforcement-learning-policy-network"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-深度策略网络-pρ-reinforcement-learning-policy-network"}},[t._v("#")]),t._v(" 1.3. 深度策略网络 pρ : (Reinforcement Learning Policy Network)")]),t._v(" "),r("ol",[r("li",[r("strong",[t._v("网络结构、输入输出与 pσ 完全一样")]),t._v("。")]),t._v(" "),r("li",[r("strong",[t._v("一开始初始化网络参数ρ = σ")])]),t._v(" "),r("li",[t._v("参数更新策略，"),r("strong",[t._v("自己和自己下棋，不断下下去直到分出胜负")]),t._v("。")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606160409_4c6a2764f1e2.png",alt:"img"}})]),t._v(" "),r("p",[t._v("上式中， "),r("strong",[t._v("pρ(at|st) 为在第t步走下一步at的概率")]),t._v("，当胜利时，Zt 等于1，否则 Zt 等于0。")]),t._v(" "),r("p",[t._v("强化学习训练策略：")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161021_bfa66eca11ff.png",alt:"img"}})]),t._v(" "),r("p",[r("strong",[t._v("对手是比他差一点，然后他提升一点，让他的对手也提升一点")]),t._v("。")]),t._v(" "),r("p",[t._v("训练细节和结果：")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606160934_d8733986c86e.png",alt:"img"}})]),t._v(" "),r("h3",{attrs:{id:"_1-4-深度策略网络pπ-rollout-policy-network"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-深度策略网络pπ-rollout-policy-network"}},[t._v("#")]),t._v(" 1.4. 深度策略网络pπ: (Rollout Policy Network)")]),t._v(" "),r("ol",[r("li",[t._v("输入特征比pσ 和 pρ少。")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606160844_69a9d3004858.png",alt:"img"}})]),t._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[t._v("网络结构更简单。")])]),t._v(" "),r("p",[t._v("换句话说，"),r("strong",[t._v("这个网络以牺牲准确率换取速度。24.2%正确率，2um一步")]),t._v("。")]),t._v(" "),r("h3",{attrs:{id:"_1-5-深度估值网络vθ-rollout-policy-network"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-5-深度估值网络vθ-rollout-policy-network"}},[t._v("#")]),t._v(" 1.5. 深度估值网络vθ: (Rollout Policy Network)")]),t._v(" "),r("ol",[r("li",[r("strong",[t._v("输入：当前棋盘状态 （与 pσ输入一样），以及执黑或执白")]),t._v("。")]),t._v(" "),r("li",[r("strong",[t._v("输出： 获胜的概率（一个0到1的数）")])]),t._v(" "),r("li",[t._v("参数更新策略：")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161238_65b0904e215c.png",alt:"img"}})]),t._v(" "),r("p",[t._v("用pρ来预测z")]),t._v(" "),r("p",[t._v("步骤：")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161343_eeb341e36bb1.png",alt:"img"}})]),t._v(" "),r("h3",{attrs:{id:"_1-6-下棋方法-蒙特卡洛树搜索-monte-carlo-tree-search"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-6-下棋方法-蒙特卡洛树搜索-monte-carlo-tree-search"}},[t._v("#")]),t._v(" 1.6. 下棋方法 -- 蒙特卡洛树搜索 （Monte Carlo Tree Search）：")]),t._v(" "),r("p",[r("strong",[t._v("多次模拟未来棋局，然后选择在模拟中获胜次数最多的走法")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161424_7a7e46cdc1d7.png",alt:"img"}})]),t._v(" "),r("p",[t._v("蒙特卡洛树搜索 （Monte Carlo Tree Search）最终确定走棋。")]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161514_db24d97892ba.png",alt:"img"}})]),t._v(" "),r("ol",[r("li",[r("strong",[t._v("一个是专家的意见，一个是真实的走法，兰姆达是其平衡因子")]),t._v("。")]),t._v(" "),r("li",[r("strong",[t._v("除以N是赋予其随机性，就是不要陷入局部走法，给其他位置以一定的机会")])])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606163241_a9fd19c3b28b.png",alt:"img"}})]),t._v(" "),r("h3",{attrs:{id:"_1-7-alphago-zero-的改进"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1-7-alphago-zero-的改进"}},[t._v("#")]),t._v(" 1.7. AlphaGo Zero 的改进")]),t._v(" "),r("ol",[r("li",[t._v("完全不需要人类棋谱，"),r("strong",[t._v("采用自己和自己下棋的方式学习")]),t._v("。")]),t._v(" "),r("li",[r("strong",[t._v("将走棋网络和估值网络合并为一个网络")]),t._v("：")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161635_3d05d0c623ed.png",alt:"img"}})]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161654_f1a5e015f598.png",alt:"img"}})]),t._v(" "),r("p",[r("strong",[t._v("自学习过程和神经网络训练过程")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161731_31b22c595598.png",alt:"img"}})]),t._v(" "),r("p",[r("strong",[t._v("标签π的生成")])]),t._v(" "),r("p",[r("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200606161822_5688fb191b98.png",alt:"img"}})]),t._v(" "),r("p",[r("strong",[t._v("目标函数")])])])}),[],!1,null,null,null);r.default=e.exports}}]);