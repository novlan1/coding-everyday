(window.webpackJsonp=window.webpackJsonp||[]).push([[250],{523:function(t,_,a){"use strict";a.r(_);var r=a(14),v=Object(r.a)({},(function(){var t=this,_=t._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("ul",[_("li",[_("a",{attrs:{href:"#1-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"}},[t._v("1. 强化学习")]),t._v(" "),_("ul",[_("li",[_("a",{attrs:{href:"#11-%E4%B8%80%E4%BA%9B%E5%AE%9A%E4%B9%89"}},[t._v("1.1. 一些定义")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#12-%E4%B8%80%E4%BA%9B%E5%81%87%E8%AE%BE"}},[t._v("1.2. 一些假设")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#13-%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8Bmarkov-decision-process"}},[t._v("1.3. 马尔科夫决策过程（Markov Decision Process）")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#14-%E5%BE%85%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"}},[t._v("1.4. 待优化目标函数")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#16-%E4%BC%B0%E5%80%BC%E5%87%BD%E6%95%B0%E5%92%8Cq%E5%87%BD%E6%95%B0"}},[t._v("1.6. 估值函数和Q函数")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#17-%E6%B1%82%E6%9C%80%E4%BD%B3%E7%AD%96%E7%95%A5%E7%9A%84%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"}},[t._v("1.7. 求最佳策略的迭代算法")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#18-deep-q-network-dqn"}},[t._v("1.8. Deep Q-Network (DQN)")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#19-q-learning%E7%9A%84%E5%8A%A3%E5%8A%BF"}},[t._v("1.9. Q-learning的劣势")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#110-policy-gradient"}},[t._v("1.10. Policy Gradient")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#111-actor-critic%E7%AE%97%E6%B3%95"}},[t._v("1.11. Actor-Critic算法")])]),t._v(" "),_("li",[_("a",{attrs:{href:"#112-%E6%80%BB%E7%BB%93"}},[t._v("1.12. 总结")])])])])]),t._v(" "),_("h2",{attrs:{id:"_1-强化学习"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-强化学习"}},[t._v("#")]),t._v(" 1. 强化学习")]),t._v(" "),_("p",[t._v("强化学习（Reinforcement Learning）与监督学习的区别：")]),t._v(" "),_("ol",[_("li",[_("strong",[t._v("训练数据中没有标签，只有奖励函数（Reward Function）")]),t._v("。")]),t._v(" "),_("li",[_("strong",[t._v("训练数据不是现成给定，而是由行为（Action）获得")]),t._v("。")]),t._v(" "),_("li",[t._v("现在的行为（Action）不仅影响后续训练数据的获得，也影响奖励函数（Reward Function）的取值。")]),t._v(" "),_("li",[t._v("训练的目的是构建一个“"),_("strong",[t._v("状态->行为")]),t._v("”的函数，其中状态（State）描述了目前内部和外部的环境，在此情况下，要使一个智能体（Agent）在某个特定的状态下，通过这个函数，决定此时应该采取的行为。希望采取这些行为后，最终获得最大的奖励函数值。")])]),t._v(" "),_("p",[t._v("监督学习目的是"),_("strong",[t._v("构建数据到标签的映射")]),t._v("，强化学习目的"),_("strong",[t._v("是构建状态到行为的函数")]),t._v("。")]),t._v(" "),_("h3",{attrs:{id:"_1-1-一些定义"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-一些定义"}},[t._v("#")]),t._v(" 1.1. 一些定义")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("Rt：t时刻的奖励函数值")])]),t._v(" "),_("li",[_("strong",[t._v("St：t时刻的状态")])]),t._v(" "),_("li",[_("strong",[t._v("At：t时刻的行为")])])]),t._v(" "),_("p",[t._v("在这里，我们假设状态数有限，行为数有限。")]),t._v(" "),_("ul",[_("li",[t._v("举例来说，"),_("strong",[t._v("St表示，在t时刻棋盘上黑子和白子的分布，以及你是执黑还是执白，下一步该你走还是不该你走等。")])]),t._v(" "),_("li",[_("strong",[t._v("At表示，在t时刻根据St，把一颗子走到了我想走的位置上")]),t._v("。")]),t._v(" "),_("li",[_("strong",[t._v("Rt在围棋中比较特殊，一直都是0，直到分出胜负，赢了为1，输了为0")]),t._v("。")])]),t._v(" "),_("h3",{attrs:{id:"_1-2-一些假设"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-一些假设"}},[t._v("#")]),t._v(" 1.2. 一些假设")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200531231401_17b30e622729.png",alt:"img"}})]),t._v(" "),_("p",[_("strong",[t._v("马尔科夫假设：t+1时刻的状态只和t时刻有关，跟t以前没有关系，在棋类游戏中很明显")]),t._v("。")]),t._v(" "),_("p",[t._v("对于下棋来说，"),_("strong",[t._v("s0是空白的棋盘，p(s0)是初始状态的概率分布")])]),t._v(" "),_("h3",{attrs:{id:"_1-3-马尔科夫决策过程-markov-decision-process"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-马尔科夫决策过程-markov-decision-process"}},[t._v("#")]),t._v(" 1.3. 马尔科夫决策过程（Markov Decision Process）")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603223753_8cd7fd1c05f3.png",alt:"img"}})]),t._v(" "),_("h3",{attrs:{id:"_1-4-待优化目标函数"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-待优化目标函数"}},[t._v("#")]),t._v(" 1.4. 待优化目标函数")]),t._v(" "),_("p",[t._v("增强学习中的待优化目标函数是累积奖励，即一段时间内的奖励函数加权平均值：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603223901_ca5a55e25ac5.png",alt:"img"}})]),t._v(" "),_("p",[t._v("在这里，"),_("strong",[t._v("GAMMA是一个衰减项")]),t._v("。")]),t._v(" "),_("p",[t._v("增强学习中已经知道的的函数是：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603223955_db07eac90207.png",alt:"img"}})]),t._v(" "),_("p",[t._v("需要学习的函数是：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224310_611a803a0c2f.png",alt:"img"}})]),t._v(" "),_("p",[t._v("要学习的函数：π(s, a) = p(a|s)，"),_("strong",[t._v("s的条件下是a的概率，学会了这个函数，整个过程就会变得自动，st=>at=>st+1=>at+1=> ...")])]),t._v(" "),_("p",[t._v("π是由s到a的映射。")]),t._v(" "),_("h3",{attrs:{id:"_1-6-估值函数和q函数"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-6-估值函数和q函数"}},[t._v("#")]),t._v(" 1.6. 估值函数和Q函数")]),t._v(" "),_("p",[t._v("根据一个决策机制（Policy），我们可以获得一条路径：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224351_ba500de34aa1.png",alt:"img"}})]),t._v(" "),_("p",[t._v("定义1：估值函数（Value Function）是衡量"),_("strong",[t._v("某个状态")]),t._v("最终"),_("strong",[t._v("能获得多少累积奖励")]),t._v("的函数:")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224419_c8a1554acb14.png",alt:"img"}})]),t._v(" "),_("p",[t._v("定义2：Q函数是衡量"),_("strong",[t._v("某个状态下采取某个行为")]),t._v("后，最终"),_("strong",[t._v("能获得多少累积奖励")]),t._v("的函数：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224430_174fe863bcaf.png",alt:"img"}})]),t._v(" "),_("p",[_("strong",[t._v("AlphaGo关键的一点在于估值函数，可以直接根据当前棋盘的状态，预言最终输赢的概率。估值函数和人们常说的，“你这盘棋输定了”，")]),t._v(" 或者“输的概率很大”，差不多。")]),t._v(" "),_("p",[t._v("估值函数和Q函数的关系")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224607_db2fb76ae411.png",alt:"img"}})]),t._v(" "),_("p",[t._v("为了更加了解方程中期望的具体形式，可以见下图，第一层的空心圆代表当前状态（state），向下连接的实心圆代表当前状态可以执行两个动作，第三层代表执行完某个动作后可能到达的状态 s’。")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224646_acffeb1ac399.png",alt:"img"}})]),t._v(" "),_("p",[t._v("根据上图得出状态价值函数公式：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224701_498b5d833c6f.png",alt:"img"}})]),t._v(" "),_("p",[t._v("我们将概率和转换为期望，上式等价于：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224736_6ecb9c3068a8.png",alt:"img"}})]),t._v(" "),_("h3",{attrs:{id:"_1-7-求最佳策略的迭代算法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-7-求最佳策略的迭代算法"}},[t._v("#")]),t._v(" 1.7. 求最佳策略的迭代算法")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224831_871ad2f3103b.png",alt:"img"}})]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603224837_f4c0432226e9.png",alt:"img"}})]),t._v(" "),_("p",[_("strong",[t._v("定住V算π，然后定住π算V，不断循环，最后结果会收敛")]),t._v("。")]),t._v(" "),_("p",[_("strong",[t._v("能让Q(S, a)最大的a，设置π(S, a)为1，其他情况都设为0")]),t._v("。道理是，"),_("strong",[t._v("通过s获得a，一定有最佳策略")]),t._v("，比如，"),_("strong",[t._v("下棋的每一步一定有最正确的下法，让最正确的取1，其他地方取0")]),t._v("。")]),t._v(" "),_("p",[t._v("这一算法的劣势：")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("对于状态数和行为数很多时，这种做法不现实")]),t._v("。")])]),t._v(" "),_("p",[t._v("例如：对一个ATARI游戏，状态数是相邻几帧所有像素的取值组合，这是一个天文数字！（ACTION数量从6到20不等）")]),t._v(" "),_("h3",{attrs:{id:"_1-8-deep-q-network-dqn"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-8-deep-q-network-dqn"}},[t._v("#")]),t._v(" 1.8. Deep Q-Network (DQN)")]),t._v(" "),_("p",[t._v("定义")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603232149_7cd6b1d90a43.png",alt:"img"}})]),t._v(" "),_("p",[t._v("在s和a确定的情况下，π的最佳策略，导致的Q*")]),t._v(" "),_("p",[t._v("则有 Bellman Equation:")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603232215_41e88c0d83cd.png",alt:"img"}})]),t._v(" "),_("p",[t._v("后面式子是"),_("strong",[t._v("s’确定情况下，遍历所有a’，找到使Q*(s’, a’)最大；在前面再对s’做平均")])]),t._v(" "),_("p",[t._v("DQN基本思路："),_("em",[_("em",[t._v("用深度神经网络来模拟Q")]),t._v("(s, a)")]),t._v("*")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603232308_f3c4c5c855e0.png",alt:"img"}})]),t._v(" "),_("p",[_("strong",[t._v("θ就是网络里所有的参数")])]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603232458_9586147c26b8.png",alt:"img"}})]),t._v(" "),_("p",[t._v("一个打飞机的Atari游戏的DQN设置：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603232719_5796e7d88741.png",alt:"img"}})]),t._v(" "),_("p",[t._v("Volodymyr Mnih et al. human-level control through deep reinforcement learning, Nature, 2015.")]),t._v(" "),_("p",[t._v("DQN算法流程：")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200603232548_eb9154065d05.png",alt:"img"}})]),t._v(" "),_("p",[t._v("随机行为的原因是，"),_("strong",[t._v("有些时候会陷入局部极值")]),t._v("。比如一个金币后面有一个洞，所以让它不断尝试。"),_("strong",[t._v("ε-贪心算法：以很小的概率去尝试，以更大的概率取最大值")]),t._v("。和理财很像，"),_("strong",[t._v("小部分钱买高风险产品，大部分钱买稳定收益产品")]),t._v("。")]),t._v(" "),_("h3",{attrs:{id:"_1-9-q-learning的劣势"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-9-q-learning的劣势"}},[t._v("#")]),t._v(" 1.9. Q-learning的劣势")]),t._v(" "),_("ul",[_("li",[t._v("在一些应用中，"),_("strong",[t._v("状态数或行为数很多时，会使Q函数非常复杂，难以收敛")]),t._v("。例如图像方面的应用，状态数是(像素值取值范围数)^(像素个数)。这样的方法，对图像和任务没有理解，单纯通过大数据来获得收敛。")]),t._v(" "),_("li",[t._v("很多程序，如下棋程序等，REWARD是最后获得（输或赢），不需要对每一个中间步骤都计算REWARD.")])]),t._v(" "),_("h3",{attrs:{id:"_1-10-policy-gradient"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-10-policy-gradient"}},[t._v("#")]),t._v(" 1.10. Policy Gradient")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200604212329_3bd7d8883072.png",alt:"img"}})]),t._v(" "),_("p",[_("strong",[t._v("如果获得好的回报就奖励，如果获得差的结果就惩罚")]),t._v("。")]),t._v(" "),_("p",[t._v("问题是，如果两个高手下棋，仅仅下错一步，就能说所有的步骤都是错的吗？")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200604212409_e48ca3e48742.png",alt:"img"}})]),t._v(" "),_("p",[t._v("V(s)可以作为预期值，比如"),_("strong",[t._v("巴萨和一个弱队比分是1:0，能说它踢得好吗？所以要把它的表现减去预期值。")])]),t._v(" "),_("h3",{attrs:{id:"_1-11-actor-critic算法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-11-actor-critic算法"}},[t._v("#")]),t._v(" 1.11. Actor-Critic算法")]),t._v(" "),_("p",[_("img",{attrs:{src:"http://img.uwayfly.com/article_mike_20200604212536_fe8f5852cf60.png",alt:"img"}})]),t._v(" "),_("ul",[_("li",[t._v("Actor "),_("strong",[t._v("基于概率选行为")])]),t._v(" "),_("li",[t._v("Critic "),_("strong",[t._v("基于 Actor 的行为评判行为的得分")])]),t._v(" "),_("li",[t._v("Actor "),_("strong",[t._v("根据 Critic 的评分修改选行为的概率")])])]),t._v(" "),_("p",[t._v("具体：")]),t._v(" "),_("ul",[_("li",[t._v("Actor（玩家）：为了玩转这个游戏得到尽量高的reward，需要一个策略：输入state，输出action，即上面的第2步。（可以用神经网络来近似这个函数。剩下的任务就是如何训练神经网络，得更高的reward。这个网络就被称为actor）")]),t._v(" "),_("li",[t._v("Critic（评委）：因为actor是基于策略policy的，所以需要critic来计算出对应actor的value，来反馈给actor，告诉他表现得好不好。所以就要使用到之前的Q值。（当然这个Q-function所以也可以用神经网络来近似。这个网络被称为critic。)")])]),t._v(" "),_("h3",{attrs:{id:"_1-12-总结"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-12-总结"}},[t._v("#")]),t._v(" 1.12. 总结")]),t._v(" "),_("ol",[_("li",[t._v("目前强化学习的发展状况：在一些特定的任务上达到人的水平或胜过人，"),_("strong",[t._v("但在一些相对复杂的任务上，例如自动驾驶等，和人存在差距")]),t._v("。")]),t._v(" "),_("li",[t._v("和真人的差距，可能不完全归咎于算法，"),_("strong",[t._v("传感器、机械的物理限制等，也是决定性因素")]),t._v("。")]),t._v(" "),_("li",[t._v("机器和人的另一差距是："),_("strong",[t._v("人有一些基本的概念")]),t._v("，依据这些概念，人能只需要很少的训练就能学会很多，但机器只有通过大规模数据，才能学会。")]),t._v(" "),_("li",[t._v("但是，机器速度快，机器永不疲倦，只要有源源不断的数据，在特定的任务上，机器做得比人好，是可以期待的。")])])])}),[],!1,null,null,null);_.default=v.exports}}]);