### 反爬的目的：
- 初级爬虫：容易弄挂网站
- 数据保护
- 商业竞争对手
- 失控的爬虫：忘记关闭等


### 反爬的常见手段（前三种是不给你`html`，后面是限制给你）
- `User-agent`反爬
- `Referer`反爬
- IP访问限制
- 必须登录（账号访问频率）
- 动态网页-加大分析难度
- 前端`js`逻辑加密和混淆
- 机器学习分析爬虫行为
- 只请求`html`，不清求`js`和`css`
- CSS代码下毒（隐藏的`a`标签，只有爬虫会访问）

