- [1. 自编码器](#1-自编码器)
  - [1.1. 几种自编码器](#11-几种自编码器)
    - [1. 欠完备自编码器](#1-欠完备自编码器)
    - [2. 正则自编码器](#2-正则自编码器)
      - [2.1 稀疏自编码器](#21-稀疏自编码器)
      - [2.2 去噪自编码器](#22-去噪自编码器)
      - [2.3 收缩自编码器](#23-收缩自编码器)

## 1. 自编码器

![img](http://img.uwayfly.com/article_mike_20200531114826_5a169729283c.png)

```
encoder：编码器；译码器；
decoder：解码器；译码器；

original  input：原输入；

初始数据流；compressed representation：压缩表示；

reconstructed input：重新输入
```



自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是**数据相关的、有损的、从样本中自动学习的**。在大部分提到自动编码器的场合，压缩和解压缩的函数是通过神经网络实现的。

1. 自动编码器是数据相关的（data-specific 或 data-dependent），这意味着自动编码器只能压缩那些与训练数据类似的数据。比如，使用人脸训练出来的自动编码器在压缩别的图片，比如树木时性能很差，因为它学习到的特征是与人脸相关的。
2. 自动编码器是有损的，意思是解压缩的输出与原来的输入相比是退化的，MP3，JPEG等压缩算法也是如此。这与无损压缩算法不同。
3. 自动编码器是从数据样本中自动学习的，这意味着很容易对指定类的输入训练出一种特定的编码器，而不需要完成任何新工作。



搭建一个自动编码器需要完成下面三样工作：**搭建编码器，搭建解码器，设定一个损失函数**，用以衡量由于压缩而损失掉的信息。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化，例如SGD。



**自编码器是一个自监督的算法，并不是一个无监督算法**。

自监督学习是监督学习的一个实例，其标签产生自输入数据。要获得一个自监督的模型，你需要一个靠谱的目标跟一个损失函数。基本上，要求模型在像素级上精确重构输入不是机器学习的兴趣所在，**学习到高级的抽象特征**才是。



目前自编码器的应用主要有两个方面，**第一是数据去噪，第二是为进行可视化而降维**。配合适当的维度和稀疏约束，自编码器可以学习到比PCA等技术更有意思的数据投影。

还可以利用自编码来进行**神经网络预训练**。对于深层网络，通过**随机初始化权重**，然后用梯度下降来训练网络，很容易发生**梯度消失**。因此训练深层网络可以先采用自编码器来训练模型的参数，然后将这些参数作为初始化参数进行有监督的训练。



对于2D的数据可视化，t-SNE（读作**tee-snee**）或许是目前最好的算法，但通常还是需要原数据的维度相对低一些。所以，可视化高维数据的一个好办法是首先使用自编码器将维度降低到较低的水平（如32维），然后再使用t-SNE将其投影在2D平面上。

拓：**三大降维技术有：PCA、t-SNE 和自编码器**



### 1.1. 几种自编码器

自编码器（autoencoder）是神经网络的一种，经过训练后能尝试将输入复制到输出。自编码器（autoencoder）内部有一个**隐藏层 h**，可以产生编码（code）表示输入。该网络可以看作由两部分组成：**一个由函数 h = f(x) 表示的编码器和一个生成重构的解码器 r = g(h)**。
如果一个自编码器只是简单地学会将处处设置为 **g(f(x)) = x**，**那么这个自编码器就没什么特别的用处**。相反，我们**不应该将自编码器设计成输入到输出完全相等**。这通常需要向自编码器强加一些约束，使它只能近似地复制，并只能复制与训练数据相似的输入。这些约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。



#### 1. 欠完备自编码器

从自编码器获得有用特征的一种方法是**限制 h的维度比 x 小**，这种编码维度小于输入维度的自编码器称为**欠完备（undercomplete）自编码器**。学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。



学习过程可以简单地描述为**最小化一个损失函数L(x,g(f(x)))**，其中 L 是一个损失函数，惩罚g(f(x)) 与 x 的差异，如**均方误差**。当解码器是线性的且 L 是均方误差，欠完备的自编码器会学习出与 PCA 相同的生成子空间。这种情况下，自编码器在训练来执行复制任务的同时学到了训据的主元子空间。如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。



#### 2. 正则自编码器

正则自编码器使用的损失函数可以**鼓励模型学习其他特性（除了将输入复制到输出）**，而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量。这些特性包括**稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性**。即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息。



所谓"鲁棒性"，是指控制系统在一定(结构，大小)的参数摄动下，维持其它某些性能的特性。



##### 2.1 稀疏自编码器

稀疏自编码器简单地在训练时结合编码层的稀疏惩罚 Ω(h) 和重构误差：L(x,g(f(x))) + Ω(h)，其中 g(h) 是解码器的输出，通常 h 是编码器的输出，即 h = f(x)。稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。稀疏正则化的自编码器必须反映训练数据集的独特统计特征，而不是简单地充当恒等函数。以这种方式训练，执行附带稀疏惩罚的复制任务可以得到能学习有用特征的模型。

稀疏自编码就是对隐层的神经元加入稀疏约束，以便**约束隐层中神经元不为0的个数**，我们希望达到用尽可能少的神经元来表示数据X，以达到稀疏降维。稀疏自编码有加速网络训练的功能。



##### 2.2 去噪自编码器

**去噪自编码器（denoising autoencoder, DAE）** 最小化L(x,g(f(˜ x)))，其中 " x" 是被某种噪声损坏的 x 的副本。因此去噪自编码器**必须撤消这些损坏**，而不是简单地复制输入。



##### 2.3 收缩自编码器

另一正则化自编码器的策略是使用一个类似稀疏自编码器中的惩罚项 Ω，

![img](http://img.uwayfly.com/article_mike_20200531120617_0684a8fa2b37.png)


这迫使模型学习一个在 x 变化小时目标也没有太大变化的函数。因为这个惩罚只对训练数据适用，它迫使自编码器学习可以反映训练数据分布信息的特征。这样正则化的自编码器被称为**收缩自编码器（contractive autoencoder, CAE）**。这种方法与去噪自编码器、流形学习和概率模型存在一定理论联系。


