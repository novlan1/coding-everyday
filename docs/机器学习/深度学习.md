- [1. 深度学习](#1-深度学习)
  - [1.1. 数据库准备](#11-数据库准备)
    - [1.1.1. Mnist:手写数字数据库 （LeCun 在1998年创造，LeNet作者）](#111-mnist手写数字数据库-lecun-在1998年创造lenet作者)
    - [1.1.2. ImageNet:（Fei-fei Li等 2007年创造）](#112-imagenetfei-fei-li等-2007年创造)
  - [1.2. 自编码器](#12-自编码器)
  - [1.3. 卷积神经网络](#13-卷积神经网络)
  - [1.4. AlexNet](#14-alexnet)
  - [1.5. AlexNet 改进](#15-alexnet-改进)
  - [1.6. Caffe的优劣](#16-caffe的优劣)
  - [1.7. 近年来流行的网络结构](#17-近年来流行的网络结构)
    - [1.7.1. VGGNet: （Simonyan and Zisserman, 2014）](#171-vggnet-simonyan-and-zisserman-2014)
    - [1.7.2. GoogLeNet: （Szegedy, 2014）](#172-googlenet-szegedy-2014)
    - [1.7.3. ResNet: （He et al, 2015）](#173-resnet-he-et-al-2015)
  - [1.8. 卷积神经网络的应用](#18-卷积神经网络的应用)
  - [1.9. Transfer Learning迁移学习](#19-transfer-learning迁移学习)

## 1. 深度学习

### 1.1. 数据库准备

#### 1.1.1. Mnist:手写数字数据库 （LeCun 在1998年创造，LeNet作者）

1. 手写数字 0-9共10类
2. 训练样本60000个，测试样本10000个。
3. 图像大小 28*28 二值图像。
4. 样例：

![img](http://img.uwayfly.com/article_mike_20200531105253_e99e22b7a7c0.png)


#### 1.1.2. ImageNet:（Fei-fei Li等 2007年创造）

1. 1000类，100多万张（2009年的规模）
2. 图片大小：正常图片大小，像素几百*几百
3. WORDNET结构，拥有多个Node（节点）。一个node（目前）含有至少500个对应物体的可供训练的图片/图像。（比如先分成动物大类，再分成猫和狗）





### 1.2. 自编码器

多层神经网络出来以后，只火了一小会。上世纪90年代到2006年，叫**人工神经网络的沉寂期**。

原因，1：**人工神经网络在小样本集上比SVM优势不明显，甚至没有优势**，2：SVM理论漂亮，人工神经网络**数学理论不够漂亮**。

经常称人工神经网络为启发性的方法，即完全没有道理，没有理论性的方法。Heuristic Method。



深度学习三剑客的坚持：

![img](http://img.uwayfly.com/article_mike_20200531110410_fe68f2375075.png)



> LeCun:“这种算法很有价值，不知为什么要放弃它。”
Hinton: “智能产生于人脑，所以从长远来说，人工智能应该像大脑系统一样工作。”


这三人从2003年开始，一起组成了“Deep learning conspiracy”,悄悄开发了10层以上的人工神经网络。


2006年是深度学习的起始年，**Hinton**在SCIENCE上发文，提出一种叫做自动编码机（Auto-encoder）的方法，部分解决了神经网络参数初始化的问题。


![img](http://img.uwayfly.com/article_mike_20200531114458_5ad86272438a.png)


**训练X到X，然后删掉最后的X（只保留编码器部分）**，并且在训练的时候保持第一层不要动，相当于**特征提取器**。实际上把X的信息压缩。训练好第M-1层后，接着训练第M层，然后**固定前M-1层参数不动**。

**one-hot vector**, 最后一层神经元个数是label个数，**比如有10类，最后一层就有十个**，即**输出维度为10维**。几乎是神经网络做识别问题的标准配置，而**不是最后输出一个数1或0这种**。





### 1.3. 卷积神经网络

卷积神经网络(Convolutional Neural Network, CNN)由LeCun在上世纪90年代提出。

LeCun Y., Bottou L., Bengio Y., and Haffner P., Gradient-based learning applied to document recognition, Proceedings of the IEEE, pp. 1-7, 1998.

**卷积核和傅里叶变换、小波变换等一样，都是乘起来，再加起来**。

![img](http://img.uwayfly.com/article_mike_20200531143949_846ea7588dca.png)


卷积神经网络用一句话描述就是：**由手工设计卷积核，变成自动学习卷积核**。

相关概念

- 原图像和卷积核都有：**width、heght、channel**，如果是视频的话，还有第四维（时间）。
- **步长stride**，是每次移动多少，包括width、height两维。
- 卷积核和原来的图像卷积后的结果称为**特征图**（**feature map**）。


图像大小：（M，N）；卷积核大小：（m，n）；步长：（u，v）；求特征图大小（K，L）数值？

- 点1：1~m；
- 点2：(1+u)~m+u；
- 点K：**(1+(K-1)*u) ~ m + (K-1)*u**
- 所以 **k <= (M-m)/u+1**，同理**L <= (N-n)/v+1**



**zero-padding**或者**padding**：补零策略，**在stride较大，漏掉了图像的边缘的部分时候使用**。比如`5*5`的图像，`2*2`的卷积核，stride是2，那么m每个维度需要补一个0.



![img](http://img.uwayfly.com/article_mike_20200531144111_53c3700a9645.png)



第一层卷积要学习的参数个数有`5*5*3*6=75*6=450`个，即**6个卷积核**。如果每个卷积核**自带一个偏置**的话，**要学习的参数个数就是`（5*5*3 + 1）*6`=456**

**如果我们用6个卷积核，就能获得6个特征图（feature map）**。**你也可以把产生的6个特征图看成一个新的“图像”**，其height, width, channel数目分别是28,28,6。



![img](http://img.uwayfly.com/article_mike_20200531144220_f856fb75f460.png)

等价于：

![img](http://img.uwayfly.com/article_mike_20200531144231_8d154f4233d8.png)


**卷积神经网络和多层神经网络没有太大区别，可以将图像卷积看成全连接网络的权值共享（weight sharing）**，同时有一些ω等于0。**其唯一本质区别就是共享了权重，比如x1、x2、x4、x5共用了ω1。**

LeNet-5网络结构如下图所示：

![img](http://img.uwayfly.com/article_mike_20200531144326_2b4d1fa953ea.png)



**做完第一层卷积后，都要做一层非线性变换，比如sigmoid、relu、tanh**。然后降采样**subsampling**，每`2*2`个格子，取平均值，从而`28*28=>14*14`。

**每做完一层卷积、降采样、全连接后，都要接一层非线性变换，否则两个线性变换层直接相连，就和一层线性变换没什么区别**。

**降采样后向传播中梯度怎么处理？乘以1/4，分配到每个格子**。

**全连接层**：本例中，**`16*5*5`共400个神经元要和下一层的120个神经元，每个都要连起来，参数个数共有`(400+1)*120`**。1是偏置，每次前面的数过来都带个偏置，就像改嫁的女人带个孩子。



```
参数个数计算：

第1层(convolutional layer): (5*5+1)*6=156
第2层(subsampling layer): 0
第3层(convolutional layer): (5*5*6+1)*16=2416
第4层(subsampling layer): 0
第5层(fully connected layer): (5*5*16+1)*120=48120
第6层(fully connected layer): (120+1)*84=10164
第7层(fully connected layer): (84+1)*10=850

参数总数：61,706
```

由上面可以看出，**卷积神经网络的参数大部分都在全连接层**。**整个网络的计算速度取决于卷积层，整个网络的参数个数取决于全连接层。**



### 1.4. AlexNet


2013 AlexNet

A. Krizhevsky, I. Sutskever and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing 25, MIT Press, Cambridge, MA, 2012.

![img](http://img.uwayfly.com/article_mike_20200531152727_562fbbacef7e.png)



**ImageNet**包含超过120万张彩色图片，属于1000个不同类别，这是目前为止最大的图像识别数据库。**Alex Krizhevsky**等人构建了一个包含65万多个神经元，待估计参数超过6000万的大规模网络，这一网络被称为AlexNet。

![img](http://img.uwayfly.com/article_mike_20200531161635_e586bf947ed0.png)



### 1.5. AlexNet 改进


（1）**以ReLU函数代替 sigmoid 或 tanh 函数**，实践证明，这样做能**使网络训练以更快速度收敛**。

![img](http://img.uwayfly.com/article_mike_20200531161204_c6fc26071864.png)


使用relu激活函数，**x<0时，y=0，使得不用每次都改变所有的值**，使收敛速度更快。同时**x>0时，导数始终为1，避免了 sigmoid 和 tanh 在深层神经网络中梯度无法向前传很远的现象**。


（2）**为降采样操作起了一个新的名字—池化（Pooling）**,意思是把邻近的像素作为一个“池子”来重新考虑。如图所示，左边所有红色的像素值可以看做是一个“池子”，经过池化操作后，变成右边的一个蓝色像素。

![img](http://img.uwayfly.com/article_mike_20200531161316_fd67653e0015.png)


在AlexNet中，提出了 **最大池化(Max Pooling)** 的概念，即对**每一个邻近像素组成的“池子”，选取像素最大值作为输出**。

在**LeNet中，池化的像素是不重叠的；而在AlexNet中进行的是有重叠的池化**。实践表明，**有重叠的最大池化能够很好的克服过拟合问题**，提升系统性能。


**平均池化时，后向传播的梯度平均分配给原来的格子**。

那么**最大池化**时，**后向传播的梯度**怎么处理？**赢者通吃，Winner takes all，最大的那个格子直接变成该梯度，其他都变成0。**

为什么最大池化比平均池化有效？

- 一，它做了降采样；
- 二，**做了非线性变换，求最大值一定是非线性的**；
- 三，**让pooling的那一层激活的神经元变少。max pooling只激活了最大的，其他的被屏蔽掉了**。**当参数很多的时候，时刻要考虑收敛的问题。每一次不要让所有的神经元都被激活，否则每次更新时variation都会很大**。



（3）随机丢弃（Dropout）

为了避免系统参数更新过快导致过拟合，每次利用训练样本更新参数时候，**随机的“丢弃”一定比例的神经元**，被丢弃的神经元将不参加训练过程，输入和输出该神经元的权重系数也不做更新。

这样每次训练时，训练的网络架构都不一样，而这些不同的网络架构却分享共同的权重系数。实验表明，**随机丢弃技术减缓了网络收敛速度，也以大概率避免了过拟合的发生。**

![img](http://img.uwayfly.com/article_mike_20200531161417_31358014565a.png)



Dropout做法是，**对每一层，每次训练时以概率p丢弃一些神经元，这样每次训练的网络都不一样**。

**训练结束后的测试流程，要用完整的网络结构**，同时对该层的所有的参数（W,b）都要乘以(1-p)。

**dropout和max pooling、relu函数的意义本质上是一样的**，**每一次每层都让有限的神经元被激活，让不能收敛的网络快速收敛**。另外，dropout让网络处于不确定的量子态，相当于同时在训练一大堆网络，最后测试时再把这一大堆网络综合起来。



（4）**增加训练样本**

尽管ImageNet的训练样本数量有超过120万幅图片，但相对于6亿待估计参数来说，训练图像仍然不够。

Alex等人采用了多种方法增加训练样本，包括：1. **将原图水平翻转**；2. **将256×256的图像随机选取224×224的片段作为输入图像**。运用上面两种方法的组合可以将一幅图像变为2048幅图像。3. 还可以对每幅图片**引入一定的噪声**，构成新的图像。

这样做可以较大规模增加训练样本，避免由于训练样本不够造成的性能损失



（5）用GPU加速训练过程

采用2片GTX 580 GPU对训练过程进行加速，由于GPU强大的并行计算能力，使得训练过程的时间缩短数十倍，哪怕这样，训练时间仍然用了六天。

![img](http://img.uwayfly.com/article_mike_20200531161539_ce73adbcfa81.png)


Alex Krizhevsky等人在ImageNet的测试集上获得了**37.5%的Top 1错误率**（即正确的类不是测试中排名第一的类的百分率）和**16.4%的Top 5错误率**（即正确的类不是测试中排名前五的类的百分率），遥遥领先。

- **TOP5错误率就是模型给出最大的5种可能，真正的结果不在这5种之中的错误率**。
- **TOP1错误率就是只给出一个答案**。


### 1.6. Caffe的优劣

优点

- 非常适合卷积神经网络做图像识别
- 预训练的model比较多
- 代码量少
- **封装比较少，源程序容易看懂，容易修改**
- **训练好的参数容易导出到其他程序文件 （如C语言）**
- 适合工业应用

缺点

- 由于是专门为卷积神经网络开发的，结构不灵活，难以进行其他应用。
- 代码写法比较僵化，每一层都要写。
- 除非修改源码，否则不能完全调节所有细节。


CAFFE2，2017年4月发布，在代码灵活性上做了一些工作。


深度学习已变成**数据和运算能力的比拼，训练样本个数、GPU**。


### 1.7. 近年来流行的网络结构


![img](http://img.uwayfly.com/article_mike_20200531165503_e8b008d02aea.png)

**各种不同网络在IMAGENET上的结果**



![img](http://img.uwayfly.com/article_mike_20200531171127_d6242f105662.png)

**不同网络识别率比较**



![img](http://img.uwayfly.com/article_mike_20200531171143_4dca73cca3fe.png)

**不同网络计算量和识别率的联合比较**


#### 1.7.1. VGGNet: （Simonyan and Zisserman, 2014）

![img](http://img.uwayfly.com/article_mike_20200531171544_73b01d00e325.png)



问题：**为何要将2个3*3卷积核叠到一起？**

**因为2个叠到一起的`3*3`卷积核，感受野（Receptive Field）是`7*7`,大致可以替代7*7卷积核的作用。但这样做可以使参数更少 ，参数比例大致为18:49**

- **感受野：就是把卷积后的点倒推到原始的那幅图上去，两个`3*3`卷积核并排的感受野和一个`7*7`的感受野相同**。
- VGGNet提高了一些识别率，但计算速度比AlexNet慢很多。因为卷积网络的计算速度主要和卷积核个数有关，VGG的卷积核很多，因此速度慢很多。


#### 1.7.2. GoogLeNet: （Szegedy, 2014）


![img](http://img.uwayfly.com/article_mike_20200531171705_a702f1d3830e.png)

- 22层
- **inception 结构**，**用一些`1*1`, `3*3`和`5*5`的小卷积核用固定方式组合到一起，来代替大的卷积核。达到增加感受野和减少参数的目的**。
- 500万参数，比ALEXNET小了12倍。
- ILSVRC’14 测试冠军（6.7% TOP 5 ERROR）

1*1的卷积核，其实只是在channel那个层面上做了加权平均。

![img](http://img.uwayfly.com/article_mike_20200531171853_4d68a6aa4766.png)



#### 1.7.3. ResNet: （He et al, 2015）

Residual net，残差网络

![img](http://img.uwayfly.com/article_mike_20200531171949_147400bff2c2.png)

- 152层
- ILSVRC’15冠军，（3.57 TOP 5 ERROR）
- 加入了前向输入机制，将前面层获得的特征图作为监督项输入到后面层。用这样的方法使深层网络训练能够收敛。



![img](http://img.uwayfly.com/article_mike_20200531172041_28004bbd02cf.png)



将浅层的输出直接加入到后面层去，促使深层网络能够表现更好。



![img](http://img.uwayfly.com/article_mike_20200531172135_d212846d082e.png)



```
ResNet训练技巧：
-- Batch Normalization
-- Xavier initialization
-- SGD + Momentum (0.9)
-- Learning Rate:0.1
-- Batch size 256
-- Weight decay 1e-5
-- No dropout
```



一般认为，一个比较深的，每层神经元个数少的网络，比一个比较浅的，每层神经元个数多的网络效果好，即**深层比浅层好**。另外，计算复杂度和识别率需要权衡。





### 1.8. 卷积神经网络的应用

- 人脸识别
- 人脸特征点检测
- 卷积神经网络压缩

**卷积神经网络压缩：有了一个神经网络，尽可能降低它的复杂度，或者降低它的存储容量，同时又不能让它的识别率下降太多**。


### 1.9. Transfer Learning迁移学习

**把一个Domain的经验迁移到另一个Domain上去**。

比如之前一个用外国人脸训练的人脸识别模型，要想让它识别亚洲人的脸，可以用10万张亚洲人的脸对模型进行微调，再用4万张身份证人脸再进行调优。

- **用训练好的模型做特征提取器**，比如**用AlexNet模型的输出，再进行神经网络或者SVM分类**，这种应用目前非常多。
- **也可以把训练好的模型当做识别器**，比如对AlexNet模型参数再进行几种水果的分类。