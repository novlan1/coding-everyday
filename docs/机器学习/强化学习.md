
- [1. 强化学习](#1-强化学习)
  - [1.1. 一些定义](#11-一些定义)
  - [1.2. 一些假设](#12-一些假设)
  - [1.3. 马尔科夫决策过程（Markov Decision Process）](#13-马尔科夫决策过程markov-decision-process)
  - [1.4. 待优化目标函数](#14-待优化目标函数)
  - [1.6. 估值函数和Q函数](#16-估值函数和q函数)
  - [1.7. 求最佳策略的迭代算法](#17-求最佳策略的迭代算法)
  - [1.8. Deep Q-Network (DQN)](#18-deep-q-network-dqn)
  - [1.9. Q-learning的劣势](#19-q-learning的劣势)
  - [1.10. Policy Gradient](#110-policy-gradient)
  - [1.11. Actor-Critic算法](#111-actor-critic算法)
  - [1.12. 总结](#112-总结)

## 1. 强化学习

强化学习（Reinforcement Learning）与监督学习的区别：

1. **训练数据中没有标签，只有奖励函数（Reward Function）**。
2. **训练数据不是现成给定，而是由行为（Action）获得**。
3. 现在的行为（Action）不仅影响后续训练数据的获得，也影响奖励函数（Reward Function）的取值。
4. 训练的目的是构建一个“**状态->行为**”的函数，其中状态（State）描述了目前内部和外部的环境，在此情况下，要使一个智能体（Agent）在某个特定的状态下，通过这个函数，决定此时应该采取的行为。希望采取这些行为后，最终获得最大的奖励函数值。



监督学习目的是**构建数据到标签的映射**，强化学习目的**是构建状态到行为的函数**。



### 1.1. 一些定义

- **Rt：t时刻的奖励函数值**
- **St：t时刻的状态**
- **At：t时刻的行为**

在这里，我们假设状态数有限，行为数有限。

- 举例来说，**St表示，在t时刻棋盘上黑子和白子的分布，以及你是执黑还是执白，下一步该你走还是不该你走等。**
- **At表示，在t时刻根据St，把一颗子走到了我想走的位置上**。
- **Rt在围棋中比较特殊，一直都是0，直到分出胜负，赢了为1，输了为0**。



### 1.2. 一些假设

![img](http://img.uwayfly.com/article_mike_20200531231401_17b30e622729.png)


**马尔科夫假设：t+1时刻的状态只和t时刻有关，跟t以前没有关系，在棋类游戏中很明显**。

对于下棋来说，**s0是空白的棋盘，p(s0)是初始状态的概率分布**


### 1.3. 马尔科夫决策过程（Markov Decision Process）

![img](http://img.uwayfly.com/article_mike_20200603223753_8cd7fd1c05f3.png)



### 1.4. 待优化目标函数

增强学习中的待优化目标函数是累积奖励，即一段时间内的奖励函数加权平均值：

![img](http://img.uwayfly.com/article_mike_20200603223901_ca5a55e25ac5.png)


在这里，**GAMMA是一个衰减项**。

增强学习中已经知道的的函数是：

![img](http://img.uwayfly.com/article_mike_20200603223955_db07eac90207.png)


需要学习的函数是：

![img](http://img.uwayfly.com/article_mike_20200603224310_611a803a0c2f.png)


要学习的函数：π(s, a) = p(a|s)，**s的条件下是a的概率，学会了这个函数，整个过程就会变得自动，st=>at=>st+1=>at+1=> ...**

π是由s到a的映射。


### 1.6. 估值函数和Q函数

根据一个决策机制（Policy），我们可以获得一条路径：

![img](http://img.uwayfly.com/article_mike_20200603224351_ba500de34aa1.png)


定义1：估值函数（Value Function）是衡量**某个状态**最终**能获得多少累积奖励**的函数:

![img](http://img.uwayfly.com/article_mike_20200603224419_c8a1554acb14.png)


定义2：Q函数是衡量**某个状态下采取某个行为**后，最终**能获得多少累积奖励**的函数：

![img](http://img.uwayfly.com/article_mike_20200603224430_174fe863bcaf.png)


**AlphaGo关键的一点在于估值函数，可以直接根据当前棋盘的状态，预言最终输赢的概率。估值函数和人们常说的，“你这盘棋输定了”，** 或者“输的概率很大”，差不多。


估值函数和Q函数的关系

![img](http://img.uwayfly.com/article_mike_20200603224607_db2fb76ae411.png)


为了更加了解方程中期望的具体形式，可以见下图，第一层的空心圆代表当前状态（state），向下连接的实心圆代表当前状态可以执行两个动作，第三层代表执行完某个动作后可能到达的状态 s’。


![img](http://img.uwayfly.com/article_mike_20200603224646_acffeb1ac399.png)


根据上图得出状态价值函数公式：

![img](http://img.uwayfly.com/article_mike_20200603224701_498b5d833c6f.png)


我们将概率和转换为期望，上式等价于：

![img](http://img.uwayfly.com/article_mike_20200603224736_6ecb9c3068a8.png)



### 1.7. 求最佳策略的迭代算法


![img](http://img.uwayfly.com/article_mike_20200603224831_871ad2f3103b.png)


![img](http://img.uwayfly.com/article_mike_20200603224837_f4c0432226e9.png)


**定住V算π，然后定住π算V，不断循环，最后结果会收敛**。

**能让Q(S, a)最大的a，设置π(S, a)为1，其他情况都设为0**。道理是，**通过s获得a，一定有最佳策略**，比如，**下棋的每一步一定有最正确的下法，让最正确的取1，其他地方取0**。

这一算法的劣势：

- **对于状态数和行为数很多时，这种做法不现实**。

例如：对一个ATARI游戏，状态数是相邻几帧所有像素的取值组合，这是一个天文数字！（ACTION数量从6到20不等）


### 1.8. Deep Q-Network (DQN)

定义

![img](http://img.uwayfly.com/article_mike_20200603232149_7cd6b1d90a43.png)

在s和a确定的情况下，π的最佳策略，导致的Q*

则有 Bellman Equation:

![img](http://img.uwayfly.com/article_mike_20200603232215_41e88c0d83cd.png)


后面式子是**s’确定情况下，遍历所有a’，找到使Q*(s’, a’)最大；在前面再对s’做平均**

DQN基本思路：**用深度神经网络来模拟Q*(s, a)**

![img](http://img.uwayfly.com/article_mike_20200603232308_f3c4c5c855e0.png)


**θ就是网络里所有的参数**


![img](http://img.uwayfly.com/article_mike_20200603232458_9586147c26b8.png)


一个打飞机的Atari游戏的DQN设置：


![img](http://img.uwayfly.com/article_mike_20200603232719_5796e7d88741.png)


Volodymyr Mnih et al. human-level control through deep reinforcement learning, Nature, 2015.


DQN算法流程：


![img](http://img.uwayfly.com/article_mike_20200603232548_eb9154065d05.png)


随机行为的原因是，**有些时候会陷入局部极值**。比如一个金币后面有一个洞，所以让它不断尝试。**ε-贪心算法：以很小的概率去尝试，以更大的概率取最大值**。和理财很像，**小部分钱买高风险产品，大部分钱买稳定收益产品**。


### 1.9. Q-learning的劣势

- 在一些应用中，**状态数或行为数很多时，会使Q函数非常复杂，难以收敛**。例如图像方面的应用，状态数是(像素值取值范围数)^(像素个数)。这样的方法，对图像和任务没有理解，单纯通过大数据来获得收敛。
- 很多程序，如下棋程序等，REWARD是最后获得（输或赢），不需要对每一个中间步骤都计算REWARD.


### 1.10. Policy Gradient

![img](http://img.uwayfly.com/article_mike_20200604212329_3bd7d8883072.png)


**如果获得好的回报就奖励，如果获得差的结果就惩罚**。

问题是，如果两个高手下棋，仅仅下错一步，就能说所有的步骤都是错的吗？

![img](http://img.uwayfly.com/article_mike_20200604212409_e48ca3e48742.png)


V(s)可以作为预期值，比如**巴萨和一个弱队比分是1:0，能说它踢得好吗？所以要把它的表现减去预期值。**


### 1.11. Actor-Critic算法


![img](http://img.uwayfly.com/article_mike_20200604212536_fe8f5852cf60.png)


- Actor **基于概率选行为**
- Critic **基于 Actor 的行为评判行为的得分**
- Actor **根据 Critic 的评分修改选行为的概率**

具体：

- Actor（玩家）：为了玩转这个游戏得到尽量高的reward，需要一个策略：输入state，输出action，即上面的第2步。（可以用神经网络来近似这个函数。剩下的任务就是如何训练神经网络，得更高的reward。这个网络就被称为actor）
- Critic（评委）：因为actor是基于策略policy的，所以需要critic来计算出对应actor的value，来反馈给actor，告诉他表现得好不好。所以就要使用到之前的Q值。（当然这个Q-function所以也可以用神经网络来近似。这个网络被称为critic。)


### 1.12. 总结

1. 目前强化学习的发展状况：在一些特定的任务上达到人的水平或胜过人，**但在一些相对复杂的任务上，例如自动驾驶等，和人存在差距**。
2. 和真人的差距，可能不完全归咎于算法，**传感器、机械的物理限制等，也是决定性因素**。
3. 机器和人的另一差距是：**人有一些基本的概念**，依据这些概念，人能只需要很少的训练就能学会很多，但机器只有通过大规模数据，才能学会。
4. 但是，机器速度快，机器永不疲倦，只要有源源不断的数据，在特定的任务上，机器做得比人好，是可以期待的。